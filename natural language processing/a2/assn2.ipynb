{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3971,"databundleVersionId":32703,"sourceType":"competition"}],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfiles = []\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        files.append(os.path.join(dirname, filename))\n\nfor file in files:\n    print(file)\n    \n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-21T15:57:32.590173Z","iopub.execute_input":"2024-04-21T15:57:32.592660Z","iopub.status.idle":"2024-04-21T15:57:33.801028Z","shell.execute_reply.started":"2024-04-21T15:57:32.592607Z","shell.execute_reply":"2024-04-21T15:57:33.799275Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/word2vec-nlp-tutorial/testData.tsv.zip\n/kaggle/input/word2vec-nlp-tutorial/sampleSubmission.csv\n/kaggle/input/word2vec-nlp-tutorial/unlabeledTrainData.tsv.zip\n/kaggle/input/word2vec-nlp-tutorial/labeledTrainData.tsv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data = pd.read_csv(files[3], delimiter = '\\t')\ntest_data = pd.read_csv(files[0], delimiter = '\\t')\n\ndatasets = [train_data, test_data]\ntitles = ['Train', 'Test']\nfor dataset, title in zip(datasets,titles):\n    display(dataset.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:57:51.143959Z","iopub.execute_input":"2024-04-21T15:57:51.144385Z","iopub.status.idle":"2024-04-21T15:57:52.424290Z","shell.execute_reply.started":"2024-04-21T15:57:51.144350Z","shell.execute_reply":"2024-04-21T15:57:52.423038Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"       id  sentiment                                             review\n0  5814_8          1  With all this stuff going down at the moment w...\n1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n3  3630_4          0  It must be assumed that those who praised this...\n4  9495_8          1  Superbly trashy and wondrously unpretentious 8...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentiment</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5814_8</td>\n      <td>1</td>\n      <td>With all this stuff going down at the moment w...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2381_9</td>\n      <td>1</td>\n      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7759_3</td>\n      <td>0</td>\n      <td>The film starts with a manager (Nicholas Bell)...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3630_4</td>\n      <td>0</td>\n      <td>It must be assumed that those who praised this...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9495_8</td>\n      <td>1</td>\n      <td>Superbly trashy and wondrously unpretentious 8...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"         id                                             review\n0  12311_10  Naturally in a film who's main themes are of m...\n1    8348_2  This movie is a disaster within a disaster fil...\n2    5828_4  All in all, this is a movie for kids. We saw i...\n3    7186_2  Afraid of the Dark left me with the impression...\n4   12128_7  A very accurate depiction of small time mob li...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12311_10</td>\n      <td>Naturally in a film who's main themes are of m...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8348_2</td>\n      <td>This movie is a disaster within a disaster fil...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5828_4</td>\n      <td>All in all, this is a movie for kids. We saw i...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7186_2</td>\n      <td>Afraid of the Dark left me with the impression...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12128_7</td>\n      <td>A very accurate depiction of small time mob li...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"### IMPORTS AND CUDA LOAD\nimport pandas as pd\nimport torch\nfrom torch.nn import Module, Embedding, GRU, Linear\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader\nfrom bs4 import BeautifulSoup\n\n# Set device to gpu if possible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using {device} device\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:57:54.672886Z","iopub.execute_input":"2024-04-21T15:57:54.673325Z","iopub.status.idle":"2024-04-21T15:57:58.485214Z","shell.execute_reply.started":"2024-04-21T15:57:54.673285Z","shell.execute_reply":"2024-04-21T15:57:58.483952Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Using cpu device\n","output_type":"stream"}]},{"cell_type":"code","source":"# Preprocess review function \n# NOTE: As it seems this function makes predictions worse!\ndef preprocess(review):\n    text = BeautifulSoup(review, \"html.parser\").get_text()\n    return \" \".join(text.lower().split())","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:58:01.722280Z","iopub.execute_input":"2024-04-21T15:58:01.723138Z","iopub.status.idle":"2024-04-21T15:58:01.729661Z","shell.execute_reply.started":"2024-04-21T15:58:01.723103Z","shell.execute_reply":"2024-04-21T15:58:01.728418Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Dataset class\nclass ReviewDataset(Dataset):\n    def __init__(self, reviews, labels, vocab):\n        self.reviews = reviews\n        self.labels = labels\n        self.vocab = vocab\n        self.max_len = 64\n\n    def __len__(self):\n        return len(self.reviews)\n\n    def __getitem__(self, idx):\n        review = self.reviews[idx]\n        tokens = [self.vocab.get(word, 0) for word in review.split()]\n        padded_tokens = tokens[:self.max_len] + [0] * (self.max_len - len(tokens))\n\n        if self.labels is not None:\n            label = self.labels[idx]\n            return torch.tensor(padded_tokens), torch.tensor(label)\n        return torch.tensor(padded_tokens)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:58:04.707758Z","iopub.execute_input":"2024-04-21T15:58:04.708170Z","iopub.status.idle":"2024-04-21T15:58:04.717972Z","shell.execute_reply.started":"2024-04-21T15:58:04.708137Z","shell.execute_reply":"2024-04-21T15:58:04.716434Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Build vocabulary, datasets and dataloaders\nvocab = {word: idx + 1 for idx, word in enumerate(set(\" \".join(train_data[\"review\"]).split()))}\nvocab[\"<pad>\"] = 0\n\ntrain_dataset = ReviewDataset(train_data[\"review\"], train_data[\"sentiment\"], vocab)\ntest_dataset = ReviewDataset(test_data[\"review\"], None, vocab)\n\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:58:07.801656Z","iopub.execute_input":"2024-04-21T15:58:07.802048Z","iopub.status.idle":"2024-04-21T15:58:09.512320Z","shell.execute_reply.started":"2024-04-21T15:58:07.802019Z","shell.execute_reply":"2024-04-21T15:58:09.511153Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Our model\n# Tried with different RNN, GRU AND LSTM\n# RNN performed noticably worse (Or i think it did and something else was the problem :d)\nclass SentimentClassifier(Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n        super().__init__()\n        self.embedding = Embedding(vocab_size, embed_dim)\n        \n        # Single layer of GRU\n        # If replaced with RNN model performs worse\n        # LSTM performance is approximately equal to GRU's performance\n        self.gru = GRU(embed_dim, hidden_dim, batch_first=True)\n        self.fc = Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        embedded = self.embedding(x)\n        _, hidden = self.gru(embedded)\n        output = self.fc(hidden.squeeze(0))\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:58:11.192556Z","iopub.execute_input":"2024-04-21T15:58:11.192950Z","iopub.status.idle":"2024-04-21T15:58:11.200619Z","shell.execute_reply.started":"2024-04-21T15:58:11.192922Z","shell.execute_reply":"2024-04-21T15:58:11.199333Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters for our model\nvocab_size = len(vocab)\nembed_dim = 64\nhidden_dim = 64 # Tried different sizes, 64 performed on average the best + it needs less time to train\noutput_dim = 1 # BINARY CLASSIFICATION (sentiment analysis)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:58:13.411283Z","iopub.execute_input":"2024-04-21T15:58:13.411696Z","iopub.status.idle":"2024-04-21T15:58:13.417650Z","shell.execute_reply.started":"2024-04-21T15:58:13.411664Z","shell.execute_reply":"2024-04-21T15:58:13.416331Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Initialize model, loss, and optimizer\nmodel = SentimentClassifier(vocab_size, embed_dim, hidden_dim, output_dim).to(device)\ncriterion = torch.nn.BCEWithLogitsLoss()\noptimizer = Adam(model.parameters())","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:58:15.151903Z","iopub.execute_input":"2024-04-21T15:58:15.152360Z","iopub.status.idle":"2024-04-21T15:58:18.219595Z","shell.execute_reply.started":"2024-04-21T15:58:15.152325Z","shell.execute_reply":"2024-04-21T15:58:18.218313Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# TRAINING LOOP\nepochs = 6\nfor epoch in range(epochs):\n    for batch_idx, (reviews, labels) in enumerate(train_loader):\n        reviews = reviews.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(reviews)\n        loss = criterion(outputs.squeeze(), labels.float())\n        loss.backward()\n        optimizer.step()\n\n        if batch_idx % 100 == 0:\n            print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T15:58:18.221485Z","iopub.execute_input":"2024-04-21T15:58:18.221994Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/6, Batch 0/391, Loss: 0.6934\nEpoch 1/6, Batch 100/391, Loss: 0.6884\nEpoch 1/6, Batch 200/391, Loss: 0.6861\nEpoch 1/6, Batch 300/391, Loss: 0.6928\nEpoch 2/6, Batch 0/391, Loss: 0.6546\nEpoch 2/6, Batch 100/391, Loss: 0.6403\nEpoch 2/6, Batch 200/391, Loss: 0.6028\nEpoch 2/6, Batch 300/391, Loss: 0.5174\nEpoch 3/6, Batch 0/391, Loss: 0.4873\nEpoch 3/6, Batch 100/391, Loss: 0.5325\nEpoch 3/6, Batch 200/391, Loss: 0.3852\nEpoch 3/6, Batch 300/391, Loss: 0.4081\nEpoch 4/6, Batch 0/391, Loss: 0.2883\n","output_type":"stream"}]},{"cell_type":"code","source":"# PREDICTION LOOP\nmodel.eval()\npredictions = []\nwith torch.no_grad():\n    for reviews in test_loader:\n        reviews = reviews.to(device)\n        outputs = model(reviews)\n        preds = torch.round(torch.sigmoid(outputs)).squeeze().cpu().numpy()\n        predictions.extend(preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save predictions\ntest_data[\"sentiment\"] = predictions\ntest_data[[\"id\", \"sentiment\"]].to_csv(\"data.csv\", index=False)\nprint(\"Predictions saved to 'data.csv'\")\n# Save model\ntorch.save(model.state_dict(), \"weights.w\")\nprint(\"Saved current model weights to 'weights.w'\")","metadata":{},"execution_count":null,"outputs":[]}]}